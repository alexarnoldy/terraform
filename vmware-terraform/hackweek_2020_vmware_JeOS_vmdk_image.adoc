////
Useful guide: https://floating.io/2019/04/iaas-terraform-and-vsphere/
////

## Current state of this document: Developing with changing perspective on the best path to follow. Continually changing and refining. 

IMPORTANT: This document is based on a fairly narrow use case in which (currently) the CaaS Platform cluster is deployed to a private host network (pre-existing or created with this document) with one thin-provisioned, 24GB virtual disk for each node. These instructions presume a licensed ESXi host and were developed using the vCenter web client. Diviations from these specs would require several, as of yet unexpored, adjustments.

## Current effort is: 
* Download the VMware JeOS image to an ESXi host and create a VM with basic settings and Internet access. 
* Install kernel-default, cloud-init and any other required software (i.e. CaaSP Management and Node), and update all software. 
* Make it into a template. 
* Deploy the Infrastructure VM from the template and attach it to a network with access to the Internet. 
* Deploy the needed infrstructure software (i.e. Chrony, dnsmasq, NAT router)

////
** Install CaaSP Management software and clone this repo into it. 
** Deploy DHCP/DNS/Chrony containers, as needed onto the Admin VM.
////
* Update the metadata.yaml and userdata.yaml cloud-init files.
* Use TF to deploy (via clone) the admin, master and worker nodes from the VM template, with cloud-init doing most of the customization.
* The hope is that this will be easier than the documented method of installing a SLES VM, as well as maintain consitency with the libvirt design and cloud-init compatibility.
## Fallback idea is to convert the OpenStack JeOS image to ESXi6 compatible vmdk and reuse the exact same cloud-init stuff in KVM and VMware.

NOTE: Should try to do as much in cloud-init as possible with KVM and VMware to increase reusability.

NOTE: Currently these instructions require a local RMT server to register against and download updates.

==== Obtain and upload the VMware JeOS image
* Go to https://download.suse.com and navigate to the "SUSE Linux Enterprise Server 15 SP1 JeOS" images
* Download the "SLES15-SP1-JeOS.x86_64-15.1-VMware-QU[1234].vmdk.xz" image 
** QU? represents the current Quartarly Updated image. 
* Use 7-Zip or another program to extract the .vmdk file from the .xz package
* Use a vSphere Client to create a folder in the desired datastore that will keep the VM template (In this guide the folder will be called "CaaSP-Node-Template")
* Upload the JeOS image into the new folder on the datastore

NOTE: Copying or moving the JeOS image with vSphere Client might leave locks on the file which will make it unusable by the VM. Uploading the image into the appropriate folders seems to avoid this issue.


////
## Current idea is to convert the OpenStack JeOS image to ESXi6 compatible vmdk so we can re-use the (hopefully exact) same cloud-init stuff in KVM and VMware.

==== Obtain, convert and upload the OpenStack JeOS image
* Go to https://download.suse.com and navigate to the "SUSE Linux Enterprise Server 15 SP1 JeOS" images
* Download the "SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-QU[1234].qcow2" image to a KVM host
** QU? represents the Quartarly Updated image. 

NOTE: Seems like need to mount the qcow2 image and enable root login to be able to update the VM template before deploying from it. Going to skip for now and not update the VM template.


* Set the QU_VERSION variable (i.e. `export QU_VERSION=QU2`) and convert the image from qcow2 to vmdk: 
----
export QU_VERSION= 
qemu-img convert -f qcow2 -O vmdk \
-o adapter_type=lsilogic,subformat=streamOptimized,compat6 \
SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-${QU_VERSION}.qcow2 \
SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-${QU_VERSION}.vmdk`
----

==== Upload the vmdk image to an ESXi host and convert it to be ESXi compatible
* Use a vSphere client to upload the vmdk image to a datastore available to the ESXi host
* Use a vSphere client to enable the ESXi Shell and SSH
* SSH to the ESXi host as root 
* cd into the datastore containing the vmdk image (usually under /vmfs/volumes/<datastore name>)
* Set the QU_VERSION variable (i.e. `export QU_VERSION=QU2`) and convert the image to be ESXi compatible
----
export QU_VERSION= 
vmkfstools -i \
SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-${QU_VERSION}.vmdk \
SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-${QU_VERSION}-ESXi-compat.vmdk
----
////


==== Create the VM template based on the JeOS image

NOTE: Use a vSphere client for the following steps. The exact steps may vary between the vSphere web client or vSphere thick client

* Create a new VM from the JeOS vmdk image based on the following options:
** (if offered) Create a new virtual machine
** (if offered) Custom Configuration
** Name: CaaSP-Node-Template
** (if offered) Select the correct datastore
** Guest Operating System: Linux
*** Select SUSE Linux Enterprise 15 SP1 (64-bit) or highest version available
** Minimum of 4 vCPUs
** Minimum Memory Size: of 4 GB
** (if shown) Delete the "New Hard Disk" with the "X" at the far right
** A single "Network Connection", preferably connected to a network that has access to an RMT server or to the Internet
*** (if offered) Adapter type: VMXNET 3 

** LSI Logic Parallel "SCSI controller"
** For the "Disk", or "New device", select "Use an existing virtual disk" or add "Existing Hard Disk"
*** Browse to the datastore and "CaaSP-Node-Template" folder then select the JeOS vmdk image

==== Power on the VM and use the console to configure and update it
* Power on the CaaSP-Node-Template VM and open its console
* Update the basic settings, including setting the root password, and accept the license
* Log into the VM console as root

TIP: If a DHCP server is not available on the network the VM is connected to, the network settings can be configured by logging in as root after configuring basic settings on the VM. The configuration can be set persistently in the /etc/sysconfig/network/ifcfg-eth0 file or temporarily using the ip command to add an IP address and default route to eth0, and using vim to add a DNS server to the /etc/resolv.conf file.

** If needed, update the network settings so the VM can reach an RMT server or the Internet
** Register SLES with an RMT server: `SUSEConnect --url <http(s)://<RMT IP or hostname>`
*** Alternately, register with SCC: `SUSEConnect -e <email address> -r <CaaSP registration code>`
** Register the containers module: `SUSEConnect -p sle-module-containers/15.1/x86_64`
** Register CaaSP with an RMT server: `SUSEConnect -p caasp/4.0/x86_64 --url <http(s)://<RMT IP or hostname>`
*** Alternately, register with SCC: `SUSEConnect -p caasp/4.0/x86_64 -r <CaaSP registration code>`
* Install the default kernel and open VM tools software: `zypper install kernel-default open-vm-tools`
** When prompted that kernel-default conflicts with kernel-default-base, choose to uninstall kernel-default-base
* Update the SLES software: `zypper update`
* Power off the template VM: `poweroff`
* Clone the template VM to the Infrastructure node (In this guide it will be named CaaSP-Infra)
* Select the `Thin Provision` virtual disk format
* DO NOT power on the VM at this time
* Select Finish

NOTE: Another network adapter will need to be added to the Infrastructgure node before powering it on, later in the process

* Power on the template VM and login as root
** Register the public cloud module: `SUSEConnect -p sle-module-public-cloud/15.1/x86_64`
* Add the software that will be needed for the clsuter nodes: `zypper install -t pattern SUSE-CaaSP-Node SUSE-CaaSP-Management`
* Install the cloud-init software: `zypper install cloud-init cloud-init-vmware-guestinfo`
* *ONLY IF* the CaaSP cluster nodes will register with a different service than the one used by the VM template (this is not common), de-register the template now: `SUSEConnect -d; SUSEConnect --cleanup`
* Clean out the identity of the template:
----
rm /etc/machine-id /var/lib/zypp/AnonymousUniqueId \
/var/lib/systemd/random-seed /var/lib/dbus/machine-id \
/var/lib/wicked/*
----
* Delete unneeded BTRFS snapshots: 
----
snapper delete $(snapper list | awk '/important=no/{print$1}')
----

* Enable cloud-init: `systemctl enable cloud-init cloud-init-local cloud-config cloud-final`
* Clear any instances of cloud-init: `sudo rm -rf /var/lib/cloud/*`

NOTE: Use the previous two steps after booting the template VM to ensure cloud-init is enabled and to clear out any instances recorded in the last boot.

* Power off the template VM: `poweroff`
* Close the console and use vSphere Client to convert the VM into a template
* Create a new Resource Pool (In this guide the Resource Pool will be called "CaaSP_RP")

==== Optionally, Use the vSphere client to create a private VM network for the CaaS Platform cluster

NOTE: Skip this step if the desired private network already exists. These steps are based on using the vSphere web client, creating a host network. Adjust, as required, for the vSphere thick client or for creating distributed networking.

* Select the ESXi host, then `Action`, `Add Networking...`
* Select `Virtual Machine Port Group`, select an existing standard vSwitch that does not have vmnics attached to it, or create a new one that has no physical network adapters assigned to it
* Set a VLAN tag, if needed, and adjust the Network label to include that tag
** Provide a Network label (In this guide it will be caasp-network-1) 

==== Configure the Infrastructure node 

* Before powering on the Infra VM, edit its settings to add another VMXNET3 network interface attached to the appropriate VM network (i.e. caasp-network-1)
* Power on the Infra VM and login to the Infra node as root via the vSphere console or ssh
* Use `ip a` to verify which interface is the private network interface (In this guide it is eth1)
* Use `yast lan` to add an IP address to the private network interface (In this guide it is 10.110.0.1/22)
* Install Git: `zypper install git-core`
* Set the hostname of the Infra node, with the domain name that will be used for CaaSP cluster (In this guide it will be infra.caasp-cluster.local): 
----
export HOSTNAME=
----
----
hostnamectl set-hostname ${HOSTNAME}
----

==== Optionally, configure the Infra node as a NAT router for the cluster network
* If there is not already a NAT router available for the cluster network, follow the instructions in this document, https://github.com/alexarnoldy/caasp-cloud_config/blob/master/NAT_router_config.adoc, to configure the Infra node as the NAT router for the cluster network

==== Optionally, create DHCP and/or DNS and/or NTP services on the Infra node

NOTE: Since dnsmasq doesn't provide DDNS services, using DHCP on the cluster network offers very limited value from a programtic perspective. Even dnsmasq's DNS services will be used primarily to consolidate the basic resolution needs of the cluster network.

IMPORTANT: Reliable NTP services are needed for the CaaSP cluster. Providing a local NTP service, like the Chrony pod as described below, is highly recommeneded.

* Podman is needed to deploy one or more of these services
** As the root user, install podman on the Infra node: `zypper install podman`

===== Deploy a dnsmasq container for DNS or DNS+DHCP services
* Create a directory to contain the dnsmasq installation and cd into it, i.e. `sudo mkdir -p /opt/ && cd $_`
* Clone this repository: `git clone https://github.com/alexarnoldy/opensuse-dnsmasq-container.git && cd opensuse-dnsmasq-container`
** Populate the *dnsmasq_hosts* file with entries for the master(s) and worker nodes
*** Set the NETWORK variable to the network portion of the subnet that will be used in the cluster network (In this guide it will be "10.110")

IMPORTANT: If this infrastructure node is used to deploy multiple CaaSP clusters, the subnet, and thus the NETWORK variable will need to be different for each.

----
export NETWORK=""
----
----
cat <<EOF> dnsmasq_hosts
${NETWORK}.2.0		master-0.caasp-cluster.local
${NETWORK}.2.1		master-1.caasp-cluster.local
${NETWORK}.2.3		master-3.caasp-cluster.local
${NETWORK}.2.0		worker-0.caasp-cluster.local
${NETWORK}.2.1		worker-1.caasp-cluster.local
${NETWORK}.2.2		worker-2.caasp-cluster.local
${NETWORK}.2.3		worker-3.caasp-cluster.local
EOF
----

NOTE: In this guide the dnsmasq.conf file parameter `domain` will be `caasp-cluster.local` and the `interface` paramter will be `eth1`. Since DHCP is not used in this example, all lines that begin with `dhcp` will be commented out.

* Follow the rest of the directions in the README.adoc file

NOTE: When running the container *DNS_IP=* will be set to eth1's IP address (In this guide it is 10.110.0.1)

===== Deploy a Chrony container for NTP services
* Create a directory to contain the Chrony installation and cd into it, i.e. `sudo mkdir -p /opt/ && cd $_`
* Clone this repository: `git clone https://github.com/alexarnoldy/opensuse-chrony-container.git && cd opensuse-chrony-container`
* Follow the directions in the README.adoc, paying close attention to the *allow* line in the chrony.conf file, which should match the subnet for this CaaSP cluster



==== Next steps should blend the official documentation and the TF automated deployment work for SUSECON



////
==== Register and update the template

NOTE: This step is option. It will reduce the total deployment time of each cluster slightly to update the software on the template. This step will only work if the template is connected to a network that has access to an RMT server (or with a registration code, access to the Internet).
////






// vim: set syntax=asciidoc:
